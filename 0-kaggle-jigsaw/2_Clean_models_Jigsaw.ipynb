{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, accuracy_score\n",
    "import string \n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (159571,)\n",
      "test  : (153164,)\n",
      "train + test : (312735,)\n"
     ]
    }
   ],
   "source": [
    "dftrain = pd.read_csv('datasources/jigsaw1/train.csv')\n",
    "dftest = pd.read_csv('datasources/jigsaw1/test.csv')\n",
    "\n",
    "traincomments = dftrain['comment_text']\n",
    "testcomments = dftest['comment_text']\n",
    "wikicomments = pd.concat([traincomments, testcomments])\n",
    "\n",
    "wikicomments = wikicomments\n",
    "print('train : {}'.format(traincomments.shape))\n",
    "print('test  : {}'.format(testcomments.shape))\n",
    "print('train + test : {}'.format(wikicomments.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Yo bitch Ja Rule is more succesful then you'll...\n",
       "1    == From RfC == \\n\\n The title is fine as it is...\n",
       "2    \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3    :If you have a look back at the source, the in...\n",
       "4            I don't anonymously edit articles at all.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:08<00:00, 19589.81it/s]\n",
      "100%|██████████| 153164/153164 [00:07<00:00, 21071.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove numeric\n",
    "#dftrain['comment_text'] = dftrain.comment_text.apply(lambda x: x.translate(str.maketrans('','', string.digits)))\n",
    "#dftest['comment_text'] = dftest.comment_text.apply(lambda x: x.translate(str.maketrans('','', string.digits)))\n",
    "# remove punctuation \n",
    "#dftrain['comment_text'] = dftrain.comment_text.apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))\n",
    "#dftest['comment_text'] = dftest.comment_text.apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))\n",
    "\n",
    "def clean_comments(df):\n",
    "    comments = []\n",
    "    for cmt in tqdm(df['comment_text']):\n",
    "        #suppression des espace et caractères non-alphabetic characters\n",
    "        comment_text = re.sub(\"[^a-zA-Z]\",\" \", cmt)\n",
    "        comment_text = re.sub(' +', ' ', comment_text.strip())\n",
    "\n",
    "        comments.append(comment_text)\n",
    "\n",
    "    return(comments)\n",
    "\n",
    "#clean comments for both train and test set\n",
    "dftrain = clean_comments(dftrain)\n",
    "dftest = clean_comments(dftest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yo bitch Ja Rule is more succesful then you ll ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me Ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time',\n",
       " 'From RfC The title is fine as it is IMO',\n",
       " 'Sources Zawe Ashton on Lapland',\n",
       " 'If you have a look back at the source the information I updated was the correct form I can only guess the source hadn t updated I shall update the information once again but thank you for your message',\n",
       " 'I don t anonymously edit articles at all']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse par  tfidf: TfidfVectorizer =  CountVectorizer + TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=30000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c0e3d9980d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word ngram vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Word ngram vector\n",
    "train_vect = vect_word.fit_transform(dftrain['comment_text'])\n",
    "test_vect = vect_word.transform(dftest['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 30000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(vect_word.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
    "                        stop_words= 'english',ngram_range=(3,6),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character n gram vector\n",
    "tr_vect_char = vect_char.fit_transform(dftrain['comment_text'])\n",
    "ts_vect_char = vect_char.transform(dftest['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aap',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abbas',\n",
       " 'abbey',\n",
       " 'abbreviated',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abc',\n",
       " 'abc news',\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'abe',\n",
       " 'abide',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'ability create',\n",
       " 'ability create articles',\n",
       " 'ability edit',\n",
       " 'abkhazia',\n",
       " 'able',\n",
       " 'able contribute',\n",
       " 'able edit',\n",
       " 'able help',\n",
       " 'able make',\n",
       " 'able provide',\n",
       " 'able read',\n",
       " 'able tell',\n",
       " 'able understand',\n",
       " 'able use',\n",
       " 'abolished',\n",
       " 'aboriginal',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abraham',\n",
       " 'abraham lincoln',\n",
       " 'abrahamic',\n",
       " 'abrasive',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutely correct',\n",
       " 'absolutely reason',\n",
       " 'absolutely right',\n",
       " 'absolutely wrong',\n",
       " 'absolutly',\n",
       " 'absorbed',\n",
       " 'abstract',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'abu',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abuse admin',\n",
       " 'abuse editing',\n",
       " 'abuse power',\n",
       " 'abuse wikipedia',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusing power',\n",
       " 'abusing power proof',\n",
       " 'abusing wikipedians',\n",
       " 'abusing wikipedians dickhead',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academic journal',\n",
       " 'academic sources',\n",
       " 'academically',\n",
       " 'academics',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acceleration',\n",
       " 'accent',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'accept apology',\n",
       " 'accept copyrighted',\n",
       " 'accept copyrighted text',\n",
       " 'acceptable',\n",
       " 'acceptable additions',\n",
       " 'acceptable additions youd',\n",
       " 'acceptable fair',\n",
       " 'acceptable fair use',\n",
       " 'acceptable source',\n",
       " 'acceptable use',\n",
       " 'acceptable use wikipedia',\n",
       " 'acceptable wikipedia',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepted appropriate',\n",
       " 'accepted appropriate article',\n",
       " 'accepted notable',\n",
       " 'accepted notable guidelines',\n",
       " 'accepted notable indicate',\n",
       " 'accepted notable page',\n",
       " 'accepted notable specific',\n",
       " 'accepted notable subjectspecific',\n",
       " 'accepted notable think',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessdate',\n",
       " 'accessed',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accompanied',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accordance fair',\n",
       " 'accordance fair use',\n",
       " 'accordance wikipedias',\n",
       " 'according',\n",
       " 'according article',\n",
       " 'according criteria',\n",
       " 'according proposed',\n",
       " 'according proposed deletion',\n",
       " 'according sources',\n",
       " 'according wikipedia',\n",
       " 'according wikipedia policy',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'account avoid',\n",
       " 'account avoid irrelevant',\n",
       " 'account blocked',\n",
       " 'account create',\n",
       " 'account created',\n",
       " 'account creation',\n",
       " 'account doing',\n",
       " 'account doing free',\n",
       " 'account dont',\n",
       " 'account ip',\n",
       " 'account just',\n",
       " 'account log',\n",
       " 'account page',\n",
       " 'account page edit',\n",
       " 'account quick',\n",
       " 'account quick free',\n",
       " 'account use',\n",
       " 'account used',\n",
       " 'account wikipedia',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurate description',\n",
       " 'accurate information',\n",
       " 'accurate say',\n",
       " 'accurate verifiable',\n",
       " 'accurate verifiable references',\n",
       " 'accurately',\n",
       " 'accurately reflect',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accusations vandalism',\n",
       " 'accuse',\n",
       " 'accuse vandalism',\n",
       " 'accused',\n",
       " 'accused sockpuppetry',\n",
       " 'accused sockpuppetry refer',\n",
       " 'accused vandalism',\n",
       " 'accusers',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusing people',\n",
       " 'accusing vandalism',\n",
       " 'ace',\n",
       " 'aces',\n",
       " 'achieve',\n",
       " 'achieve consensus',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acids',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledged adding',\n",
       " 'acknowledged adding source',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'aclass',\n",
       " 'acorn',\n",
       " 'acoustic',\n",
       " 'acquainted',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisition',\n",
       " 'acres',\n",
       " 'acronym',\n",
       " 'acronyms',\n",
       " 'acs',\n",
       " 'act',\n",
       " 'act like',\n",
       " 'act vandalism',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'acting good',\n",
       " 'acting good faith',\n",
       " 'acting like',\n",
       " 'action',\n",
       " 'action taken',\n",
       " 'actionable',\n",
       " 'actions',\n",
       " 'actions taken',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'activity additional',\n",
       " 'activity additional tips',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'acts kindness',\n",
       " 'actual',\n",
       " 'actual article',\n",
       " 'actual content',\n",
       " 'actual facts',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'actually believe',\n",
       " 'actually correct',\n",
       " 'actually did',\n",
       " 'actually does',\n",
       " 'actually doing',\n",
       " 'actually dont',\n",
       " 'actually good',\n",
       " 'actually happened',\n",
       " 'actually im',\n",
       " 'actually just',\n",
       " 'actually know',\n",
       " 'actually like',\n",
       " 'actually look',\n",
       " 'actually make',\n",
       " 'actually quite',\n",
       " 'actually read',\n",
       " 'actually said',\n",
       " 'actually say',\n",
       " 'actually says',\n",
       " 'actually think',\n",
       " 'actually true',\n",
       " 'actually trying',\n",
       " 'actually use',\n",
       " 'actually used',\n",
       " 'acupuncture',\n",
       " 'ad',\n",
       " 'ad hominem',\n",
       " 'ad hominem attacks',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adapted',\n",
       " 'add',\n",
       " 'add article',\n",
       " 'add article leave',\n",
       " 'add articles',\n",
       " 'add citation',\n",
       " 'add citations',\n",
       " 'add comment',\n",
       " 'add comments',\n",
       " 'add comments categorys',\n",
       " 'add comments entry',\n",
       " 'add comments review',\n",
       " 'add commercial',\n",
       " 'add commercial links',\n",
       " 'add content',\n",
       " 'add content instead',\n",
       " 'add contextual',\n",
       " 'add contextual material',\n",
       " 'add copyright',\n",
       " 'add copyright tag',\n",
       " 'add deleting',\n",
       " 'add deleting original',\n",
       " 'add details',\n",
       " 'add discussion',\n",
       " 'add dont',\n",
       " 'add fact',\n",
       " 'add following',\n",
       " 'add image',\n",
       " 'add inappropriate',\n",
       " 'add inappropriate external',\n",
       " 'add info',\n",
       " 'add information',\n",
       " 'add information article',\n",
       " 'add just',\n",
       " 'add link',\n",
       " 'add links',\n",
       " 'add list',\n",
       " 'add material',\n",
       " 'add new',\n",
       " 'add nonsense',\n",
       " 'add nonsense wikipedia',\n",
       " 'add note',\n",
       " 'add opinion',\n",
       " 'add page',\n",
       " 'add page existing',\n",
       " 'add page just',\n",
       " 'add page leave',\n",
       " 'add proper',\n",
       " 'add proper copyright',\n",
       " 'add question',\n",
       " 'add question village',\n",
       " 'add reference',\n",
       " 'add references',\n",
       " 'add replaceable',\n",
       " 'add replaceable fair',\n",
       " 'add section',\n",
       " 'add source',\n",
       " 'add sources',\n",
       " 'add talk',\n",
       " 'add template',\n",
       " 'add text',\n",
       " 'add think',\n",
       " 'add think useful',\n",
       " 'add unreasonable',\n",
       " 'add unreasonable contents',\n",
       " 'add unsourced',\n",
       " 'add unsourced original',\n",
       " 'add value',\n",
       " 'add wikipedia',\n",
       " 'added',\n",
       " 'added article',\n",
       " 'added article discuss',\n",
       " 'added articles',\n",
       " 'added citation',\n",
       " 'added comment',\n",
       " 'added content',\n",
       " 'added couple',\n",
       " 'added createdtook',\n",
       " 'added createdtook picture',\n",
       " 'added image',\n",
       " 'added info',\n",
       " 'added information',\n",
       " 'added just',\n",
       " 'added link',\n",
       " 'added links',\n",
       " 'added list',\n",
       " 'added material',\n",
       " 'added new',\n",
       " 'added note',\n",
       " 'added page',\n",
       " 'added prod',\n",
       " 'added prod template',\n",
       " 'added reference',\n",
       " 'added references',\n",
       " 'added removed',\n",
       " 'added section',\n",
       " 'added source',\n",
       " 'added sources',\n",
       " 'added tag',\n",
       " 'added talk',\n",
       " 'added talk contribs',\n",
       " 'added template',\n",
       " 'added text',\n",
       " 'added wikipedia',\n",
       " 'addendum',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'adding article',\n",
       " 'adding article just',\n",
       " 'adding citations',\n",
       " 'adding citations independent',\n",
       " 'adding citations reliable',\n",
       " 'adding comments',\n",
       " 'adding comments wikipediaarticles',\n",
       " 'adding comments wikipediamiscellany',\n",
       " 'adding content',\n",
       " 'adding inappropriate',\n",
       " 'adding inappropriate external',\n",
       " 'adding information',\n",
       " 'adding link',\n",
       " 'adding links',\n",
       " 'adding material',\n",
       " 'adding new',\n",
       " 'adding nonsense',\n",
       " 'adding nonsense wikipedia',\n",
       " 'adding note',\n",
       " 'adding note articles',\n",
       " 'adding note talk',\n",
       " 'adding page',\n",
       " 'adding page just',\n",
       " 'adding page nominated',\n",
       " 'adding references',\n",
       " 'adding section',\n",
       " 'adding source',\n",
       " 'adding source add',\n",
       " 'adding talk',\n",
       " 'adding talk page',\n",
       " 'adding text',\n",
       " 'adding unreferenced',\n",
       " 'adding unsourced',\n",
       " 'addition',\n",
       " 'addition article',\n",
       " 'addition boilerplate',\n",
       " 'addition boilerplate fair',\n",
       " 'additional',\n",
       " 'additional comments',\n",
       " 'additional information',\n",
       " 'additional sources',\n",
       " 'additional tips',\n",
       " 'additional tips sign',\n",
       " 'additionally',\n",
       " 'additionally users',\n",
       " 'additionally users perform',\n",
       " 'additions',\n",
       " 'additions article',\n",
       " 'additions links',\n",
       " 'additions links wikipedia',\n",
       " 'additions youd',\n",
       " 'additions youd like',\n",
       " 'address',\n",
       " 'address associated',\n",
       " 'address associated original',\n",
       " 'address blocked',\n",
       " 'address concerns',\n",
       " 'address didnt',\n",
       " 'address didnt make',\n",
       " 'address issue',\n",
       " 'address issues',\n",
       " 'address issues raised',\n",
       " 'address logging',\n",
       " 'address logging does',\n",
       " 'address shared',\n",
       " 'address shared multiple',\n",
       " 'address used',\n",
       " 'address used identify',\n",
       " 'address username',\n",
       " 'address username youre',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adelaide',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhd',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adherents',\n",
       " 'adhering',\n",
       " 'adi',\n",
       " 'adjacent',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustments',\n",
       " 'adl',\n",
       " 'adler',\n",
       " 'admi',\n",
       " 'admin',\n",
       " 'admin abuse',\n",
       " 'admin bad',\n",
       " 'admin bad admin',\n",
       " 'admin block',\n",
       " 'admin blocked',\n",
       " 'admin did',\n",
       " 'admin doesnt',\n",
       " 'admin dont',\n",
       " 'admin help',\n",
       " 'admin im',\n",
       " 'admin just',\n",
       " 'admin like',\n",
       " 'admin noticeboard',\n",
       " 'admin power',\n",
       " 'admin power trip',\n",
       " 'admin powers',\n",
       " 'admin privileges',\n",
       " 'admin tools',\n",
       " 'administered',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'administrative action',\n",
       " 'administrator',\n",
       " 'administrator attention',\n",
       " 'administrator attention policies',\n",
       " 'administrator questions',\n",
       " 'administrator questions feel',\n",
       " 'administrator wikipedia',\n",
       " 'administratorprick',\n",
       " 'administratorprick administratorprick',\n",
       " 'administratorprick administratorprick administratorprick',\n",
       " 'administrators',\n",
       " 'administrators noticeboard',\n",
       " 'administrators wait',\n",
       " 'administrators wait add',\n",
       " 'administrators wikipedia',\n",
       " 'admins',\n",
       " 'admins dont',\n",
       " 'admins eat',\n",
       " 'admins eat shit',\n",
       " 'admins like',\n",
       " 'admins request',\n",
       " 'admins request copy',\n",
       " 'adminship',\n",
       " 'adminstrator',\n",
       " 'admiral',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admixture',\n",
       " 'adolescent',\n",
       " 'adolf',\n",
       " 'adolf hitler',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopted experienced',\n",
       " 'adopted experienced editor',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adress',\n",
       " 'adrian',\n",
       " 'adrift',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adverse',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisement messages',\n",
       " 'advertisement messages text',\n",
       " 'advertisements',\n",
       " 'advertising',\n",
       " 'advertising contest',\n",
       " 'advertising contest tagging',\n",
       " 'advertising mere',\n",
       " 'advertising mere collection',\n",
       " 'advertising promotes',\n",
       " 'advertising promotes company',\n",
       " 'advertising promotion',\n",
       " 'advertising promotion inappropriate',\n",
       " 'advertising promotion wikipedia',\n",
       " 'advice',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'advocating',\n",
       " 'ae',\n",
       " 'aegean',\n",
       " 'aerial',\n",
       " 'aeropagitica',\n",
       " 'aesthetic',\n",
       " 'aesthetics',\n",
       " 'af',\n",
       " 'afaik',\n",
       " 'afc',\n",
       " 'afd',\n",
       " 'afd discussion',\n",
       " 'afd nomination',\n",
       " 'afd page',\n",
       " 'afd process',\n",
       " 'afds',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'affiliated',\n",
       " 'affiliated links',\n",
       " 'affiliated links exist',\n",
       " 'affiliates',\n",
       " 'affiliation',\n",
       " 'affiliations',\n",
       " 'affirm',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affirmative action',\n",
       " 'affix',\n",
       " 'affix template',\n",
       " 'affix template page',\n",
       " 'afford',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afghans',\n",
       " 'afl',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'afraid dont',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'african american',\n",
       " 'african american vernacular',\n",
       " 'african americans',\n",
       " 'africanamerican',\n",
       " 'africans',\n",
       " 'afrocentric',\n",
       " 'afterall',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afterward',\n",
       " 'ag',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agendas',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agf',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressor',\n",
       " 'agk',\n",
       " 'agnes',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agree article',\n",
       " 'agree completely',\n",
       " 'agree deletion',\n",
       " 'agree deletion article',\n",
       " 'agree disagree',\n",
       " 'agree does',\n",
       " 'agree dont',\n",
       " 'agree just',\n",
       " 'agree point',\n",
       " 'agree talk',\n",
       " 'agree think',\n",
       " 'agree wikipedia',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'agrees',\n",
       " 'agricultural',\n",
       " 'agriculture',\n",
       " 'agw',\n",
       " 'ah',\n",
       " 'ah yes',\n",
       " 'aha',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha lmao',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha lmao reported',\n",
       " 'ahead',\n",
       " 'ahead add',\n",
       " 'ahead block',\n",
       " 'ahead delete',\n",
       " 'ahead make',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahmad',\n",
       " 'ahmadinejad',\n",
       " 'ahmed',\n",
       " 'ahole',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aids aids',\n",
       " 'aids aids aids',\n",
       " 'aids aids aidsaids',\n",
       " 'aids aidsaids',\n",
       " 'aids aidsaids aids',\n",
       " 'aidsaids',\n",
       " 'aidsaids aids',\n",
       " 'aidsaids aids aids',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'air force',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airing',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airways',\n",
       " 'aiv',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'aka red',\n",
       " 'aka red pen',\n",
       " 'akbar',\n",
       " 'akin',\n",
       " 'akins',\n",
       " 'al',\n",
       " 'al qaeda',\n",
       " 'alabama',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albanians',\n",
       " 'albany',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'albert einstein',\n",
       " 'alberta',\n",
       " 'album',\n",
       " 'album cover',\n",
       " 'album covers',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholism',\n",
       " 'alden',\n",
       " 'aleppo',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alerting',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexander',\n",
       " 'alexander great',\n",
       " 'alexandria',\n",
       " 'alexandrovich',\n",
       " 'alexikoua',\n",
       " 'alf',\n",
       " 'alfred',\n",
       " 'algebra',\n",
       " 'algeria',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'aliases',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'aligncenter',\n",
       " 'aligned',\n",
       " 'alignment',\n",
       " 'alignright',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'alleging',\n",
       " 'allemande',\n",
       " 'allen',\n",
       " 'alliance',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'allmusic',\n",
       " 'allow',\n",
       " 'allow people',\n",
       " 'allow wikipedia',\n",
       " 'allowed',\n",
       " 'allowed continue',\n",
       " 'allowed edit',\n",
       " 'allowed use',\n",
       " 'allowed wikipedia',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allready',\n",
       " 'alltime',\n",
       " 'ally',\n",
       " 'almighty',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'alphabetical',\n",
       " 'alphabetical order',\n",
       " 'alqaeda',\n",
       " 'alright',\n",
       " 'als',\n",
       " 'alt',\n",
       " 'altaic',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alter search',\n",
       " 'alter search engine',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternative medicine',\n",
       " 'alternative rock',\n",
       " 'alternatively',\n",
       " 'alternatively choose',\n",
       " 'alternatively choose replace',\n",
       " 'alternatives',\n",
       " 'alternatives deleted',\n",
       " 'alternatives deleted days',\n",
       " 'alternatives deleted week',\n",
       " 'altetendekrabbe',\n",
       " 'altetendekrabbe malik',\n",
       " 'altetendekrabbe malik shabazzs',\n",
       " 'altitude',\n",
       " 'alto',\n",
       " 'altogether',\n",
       " 'aluminum',\n",
       " 'alumni',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amanda knox',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'amazoncom',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambrose',\n",
       " 'ambush',\n",
       " 'amd',\n",
       " 'amen',\n",
       " 'amend',\n",
       " 'amended',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'america',\n",
       " 'american',\n",
       " 'american english',\n",
       " 'american history',\n",
       " 'american people',\n",
       " 'american scum',\n",
       " 'american scum piece',\n",
       " 'american vernacular',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'americorps',\n",
       " 'americorps nccc',\n",
       " 'ammunition',\n",
       " 'amnesty',\n",
       " 'amos',\n",
       " 'amounts',\n",
       " 'ample',\n",
       " 'amsterdam',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'anal rape',\n",
       " 'anal rape anal',\n",
       " 'anal sex',\n",
       " 'analogies',\n",
       " 'analogous',\n",
       " 'analogy',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'analyzing',\n",
       " 'anarchism',\n",
       " 'anarchist',\n",
       " 'anarchists',\n",
       " 'anarchy',\n",
       " 'anatolia',\n",
       " 'anatomy',\n",
       " 'ancestor',\n",
       " 'ancestors',\n",
       " 'ancestral',\n",
       " 'ancestry',\n",
       " 'ancestryfuckoffjewish',\n",
       " 'ancestryfuckoffjewish ancestryfuckoffjewish',\n",
       " 'ancestryfuckoffjewish ancestryfuckoffjewish ancestryfuckoffjewish',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'ancient greece',\n",
       " 'ancient greek',\n",
       " 'ancient history',\n",
       " 'ancient india',\n",
       " 'andemu',\n",
       " 'anderson',\n",
       " 'andhra',\n",
       " 'andhra pradesh',\n",
       " 'andor',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'andys',\n",
       " 'andys edits',\n",
       " 'andythegrump',\n",
       " 'anecdotal',\n",
       " 'anecdote',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angela billj',\n",
       " 'angela billj dumbass',\n",
       " 'angeles',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angered',\n",
       " 'angle',\n",
       " 'angler',\n",
       " 'angler john',\n",
       " 'angler john bailey',\n",
       " 'angles',\n",
       " 'anglican',\n",
       " 'anglo',\n",
       " 'anglosaxon',\n",
       " 'angry',\n",
       " 'ani',\n",
       " 'ani discussion',\n",
       " 'ani thread',\n",
       " 'animal',\n",
       " 'animal rights',\n",
       " 'animals',\n",
       " 'animals maher',\n",
       " 'animals maher link',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'ann',\n",
       " 'ann coulter',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annexation',\n",
       " 'annexed',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_word.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 30000), (159571, 40000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vect.shape, tr_vect_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.hstack([train_vect, tr_vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = sparse.hstack([test_vect, ts_vect_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "#target_col = ['toxic']\n",
    "y = dftrain[target_col]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#prd = np.zeros((x_test.shape[0],y.shape[1]))\n",
    "cv_score =[]\n",
    "    #training \n",
    "lr = LogisticRegression(C=2,random_state = 42,class_weight = 'balanced')\n",
    "print('Column:{''}'.format(\"toxic\")) \n",
    "lr.fit(X,y[\"toxic\"])\n",
    "    \n",
    "# prédiction\n",
    "pred =  lr.predict(X)\n",
    "\n",
    "pred_pro = lr.predict_proba(X)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test =  lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707778982396551"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y['toxic'],pred)\n",
    "0.9707778982396551\n",
    "  9705084257164522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column:toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy_score :0.9727080735221312\n",
      "\n",
      "Confusion matrix\n",
      " [[139987   4290]\n",
      " [    65  15229]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98    144277\n",
      "           1       0.78      1.00      0.87     15294\n",
      "\n",
      "   micro avg       0.97      0.97      0.97    159571\n",
      "   macro avg       0.89      0.98      0.93    159571\n",
      "weighted avg       0.98      0.97      0.97    159571\n",
      "\n",
      "Column:severe_toxic\n",
      "\n",
      " accuracy_score :0.9866015754742403\n",
      "\n",
      "Confusion matrix\n",
      " [[155838   2138]\n",
      " [     0   1595]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    157976\n",
      "           1       0.43      1.00      0.60      1595\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    159571\n",
      "   macro avg       0.71      0.99      0.80    159571\n",
      "weighted avg       0.99      0.99      0.99    159571\n",
      "\n",
      "Column:obscene\n",
      "\n",
      " accuracy_score :0.9870653188862638\n",
      "\n",
      "Confusion matrix\n",
      " [[149060   2062]\n",
      " [     2   8447]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    151122\n",
      "           1       0.80      1.00      0.89      8449\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    159571\n",
      "   macro avg       0.90      0.99      0.94    159571\n",
      "weighted avg       0.99      0.99      0.99    159571\n",
      "\n",
      "Column:threat\n",
      "\n",
      " accuracy_score :0.9971047370762858\n",
      "\n",
      "Confusion matrix\n",
      " [[158631    462]\n",
      " [     0    478]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    159093\n",
      "           1       0.51      1.00      0.67       478\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    159571\n",
      "   macro avg       0.75      1.00      0.84    159571\n",
      "weighted avg       1.00      1.00      1.00    159571\n",
      "\n",
      "Column:insult\n",
      "\n",
      " accuracy_score :0.9781100575919184\n",
      "\n",
      "Confusion matrix\n",
      " [[148202   3492]\n",
      " [     1   7876]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    151694\n",
      "           1       0.69      1.00      0.82      7877\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    159571\n",
      "   macro avg       0.85      0.99      0.90    159571\n",
      "weighted avg       0.98      0.98      0.98    159571\n",
      "\n",
      "Column:identity_hate\n",
      "\n",
      " accuracy_score :0.9893715023406509\n",
      "\n",
      "Confusion matrix\n",
      " [[156470   1696]\n",
      " [     0   1405]]\n",
      "\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    158166\n",
      "           1       0.45      1.00      0.62      1405\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    159571\n",
      "   macro avg       0.73      0.99      0.81    159571\n",
      "weighted avg       1.00      0.99      0.99    159571\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"     \\n    # ROC\\n    pred_pro = lr.predict_proba(X)[:,1]\\n    frp,trp,thres = roc_curve(y[col],pred_pro)\\n    auc_val =auc(frp,trp)\\n    plt.figure(figsize=(5,3))\\n    plt.plot([0,1],[0,1],color='b')\\n    \\n    plt.plot(frp,trp,color='r',label= 'AUC = %.2f'%auc_val)\\n    plt.legend(loc='lower right')\\n    plt.xlabel('True positive rate')\\n    plt.ylabel('False positive rate')\\n    plt.title('ROC : {}'.format(col))\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prd = np.zeros((x_test.shape[0],y.shape[1]))\n",
    "cv_score =[]\n",
    "for i,col in enumerate(target_col):\n",
    "    #training \n",
    "    lr = LogisticRegression(C=2,random_state = 42,class_weight = 'balanced')\n",
    "    print('Column:{''}'.format(col)) \n",
    "    lr.fit(X,y[col])\n",
    "    pred =  lr.predict(X)\n",
    "    \n",
    "        \n",
    "    # prédiction\n",
    "\n",
    "    acc_res = accuracy_score(y[col],pred)\n",
    "    print('\\n accuracy_score :{}'.format(acc_res))\n",
    "    print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "    sns.heatmap(confusion_matrix(y[col],pred), annot=True, fmt='d')\n",
    "    print('\\nclassification_report\\n', classification_report(y[col],pred))\n",
    "\"\"\"     \n",
    "    # ROC\n",
    "    pred_pro = lr.predict_proba(X)[:,1]\n",
    "    frp,trp,thres = roc_curve(y[col],pred_pro)\n",
    "    auc_val =auc(frp,trp)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot([0,1],[0,1],color='b')\n",
    "    \n",
    "    plt.plot(frp,trp,color='r',label= 'AUC = %.2f'%auc_val)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('True positive rate')\n",
    "    plt.ylabel('False positive rate')\n",
    "    plt.title('ROC : {}'.format(col))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_vect, test_vect, tr_vect_char, ts_vect_char\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_col:\n",
    "    print(100 * '=')\n",
    "    print(\"Column:\",col)\n",
    "    pred =  lr.predict(X)\n",
    "    print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "    print('\\nclassification_report\\n', classification_report(y[col],pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_col:    \n",
    "    pred_pro = lr.predict_proba(X)[:,1]\n",
    "    frp,trp,thres = roc_curve(y[col],pred_pro)\n",
    "    auc_val =auc(frp,trp)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot([0,1],[0,1],color='b')\n",
    "    \n",
    "    plt.plot(frp,trp,color='r',label= 'AUC = %.2f'%auc_val)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('True positive rate')\n",
    "    plt.ylabel('False positive rate')\n",
    "    plt.title('ROC : {}'.format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_1 = pd.DataFrame(prd,columns=y.columns)\n",
    "submit = pd.concat([dftest['id'],prd_1],axis=1)\n",
    "#submit.to_csv('toxic_lr.csv.gz',compression='gzip',index=False)\n",
    "#submit.to_csv('toxic_lr.csv',index=False)\n",
    "submit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                   stratify= y,\n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 40\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "#classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "#classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state=random_state))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "#classifiers.append(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers:\n",
    "    cv_results.append(cross_val_score(classifier, \n",
    "                                     X_train,\n",
    "                                     np.ravel(y_train),\n",
    "                                     scoring='accuracy',\n",
    "                                     cv=3,\n",
    "                                     n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means, \n",
    "                       \"CrossValErros\":cv_std,\n",
    "                      \"Algorithm\": [\n",
    "                                   \"Logistic Reg\",\n",
    "                                   \"Random Forest\",\n",
    "                                   \"Naive Bayes\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValErros</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.959525</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>Logistic Reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.949838</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.945371</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>Naive Bayes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValErros      Algorithm\n",
       "0       0.959525       0.000628   Logistic Reg\n",
       "1       0.949838       0.000521  Random Forest\n",
       "2       0.945371       0.000918    Naive Bayes"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Cross Val Scores')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "g = sns.barplot(\"CrossValMeans\", \"Algorithm\", data = cv_res)\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g.set_title(\"Cross Val Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValErros</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.945371</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>Naive Bayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.949838</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.959525</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>Logistic Reg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValErros      Algorithm\n",
       "2       0.945371       0.000918    Naive Bayes\n",
       "1       0.949838       0.000521  Random Forest\n",
       "0       0.959525       0.000628   Logistic Reg"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res.sort_values(\"CrossValMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingC = VotingClassifier(estimators=[\n",
    "    (\"LogReg\", classifiers[1]),\n",
    "    (\"Naive Bayes\", classifiers[2]),\n",
    "    (\"RF\", classifiers[0])\n",
    "], voting = \"hard\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "WRITEBACKIFCOPY base is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/voting_classifier.py\", line 32, in _parallel_fit_estimator\n    estimator.fit(X, y)\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\", line 1288, in fit\n    accept_large_sparse=solver != 'liblinear')\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 756, in check_X_y\n    estimator=estimator)\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 517, in check_array\n    accept_large_sparse=accept_large_sparse)\n  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 339, in _ensure_sparse_format\n    spmatrix = spmatrix.astype(dtype)\n  File \"/anaconda3/lib/python3.7/site-packages/scipy/sparse/data.py\", line 71, in astype\n    self._deduped_data().astype(dtype, casting=casting, copy=copy),\n  File \"/anaconda3/lib/python3.7/site-packages/scipy/sparse/data.py\", line 34, in _deduped_data\n    self.sum_duplicates()\n  File \"/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\", line 1028, in sum_duplicates\n    self.sort_indices()\n  File \"/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\", line 1074, in sort_indices\n    self.indices, self.data)\nValueError: WRITEBACKIFCOPY base is read-only\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-71bfa2fe47f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvotingC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    198\u001b[0m             delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n\u001b[1;32m    199\u001b[0m                                              sample_weight=sample_weight)\n\u001b[0;32m--> 200\u001b[0;31m             for clf in clfs if clf is not None)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_estimators_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: WRITEBACKIFCOPY base is read-only"
     ]
    }
   ],
   "source": [
    "votingC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(votingC,X_train, np.ravel(y_train), scoring=\"accuracy\", cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
