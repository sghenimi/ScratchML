{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from pickle import dump\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, cleaning and saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/Users/sghenimi/SANDBOX/ScratchML/2019-04-16-nlp_sarcasm\"\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on charge les documents en mémoire\n",
    "def to_load_doc_json(filename):\n",
    "    data = pd.read_json(filename, lines=True)\n",
    "    data = data.drop(\"article_link\", axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer une liste de lignes\n",
    "def to_clean(data):\n",
    "    data_clean = pd.DataFrame(columns=data.columns)\n",
    "    # On prépare la focntion regex pour filtrer certains caractères\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # preparer la table de traduction pour retirer la ponctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for index, line in data.iterrows():\n",
    "        text = line[0]\n",
    "        # on normalise les caractères en ascii\n",
    "        text = normalize('NFD', text).encode('ascii', 'ignore')\n",
    "        text = text.decode('UTF-8')\n",
    "        # on produit des tokens de mots en utilisant les espaces comme séparateurs\n",
    "        text = text.split()\n",
    "        # On passe en minuscule\n",
    "        text = [word.lower() for word in text]\n",
    "        # on enleve la ponctuation de chaque token\n",
    "        text = [word.translate(table) for word in text]\n",
    "        # on supprime tous les caractères non imprimables\n",
    "        text = [re_print.sub('', w) for w in text]\n",
    "        # On supprime les tokens contenant des chiffres\n",
    "        text = [word for word in text if word.isalpha()]\n",
    "\t\t# enfin on enregistre les token comme des chaines de caractères\n",
    "        data_clean = data_clean.append({\n",
    "                    data.columns[0]:' '.join(text),\n",
    "                    data.columns[1]:line[1]\n",
    "                }, ignore_index=True)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sauvegarde un fichier de phrases nettoyées\n",
    "def to_save_cleaned_data(data, filename):\n",
    "    dump(data, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = to_load_doc_json(\"data/data-raw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = to_clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/data-cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "to_save_cleaned_data(data, \"data/data-cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from data.const import N_ROWS, TEST_SIZE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des données nettoyées\n",
    "def load_clean_data(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# sauvegarde d'une liste de phrase nettoyées dans un fichier\n",
    "def save_clean_data(data, filename):\n",
    "    dump(data, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement et réduction des données nettoyées\n",
    "data = load_clean_data(\"data/data-cleaned.pkl\")\n",
    "data = data[:N_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5341"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# séparation en train / test\n",
    "test_size = int(TEST_SIZE*data.shape[0])\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, \n",
    "                               test_size=test_size, \n",
    "                               random_state=0, \n",
    "                               stratify=data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21368, 2), (5341, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output/data-cleaned-r26709-both.pkl\n",
      "Saved: output/data-cleaned-r26709-train.pkl\n",
      "Saved: output/data-cleaned-r26709-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# sauvegarde dans deux fichiers\n",
    "save_clean_data(data,  'output/data-cleaned-r{}-both.pkl'.format(N_ROWS))\n",
    "save_clean_data(train, 'output/data-cleaned-r{}-train.pkl'.format(N_ROWS))\n",
    "save_clean_data(test,  'output/data-cleaned-r{}-test.pkl'.format(N_ROWS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
